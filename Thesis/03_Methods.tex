\section{Methods}
\label{sec:Methods}

\subsection{Requirements \& Comparison of FL Solutions}
\label{subsec:MethodsRequirements}

% Deriving Requirements
Implementations of FL solutions in a real-world medical environment need to fulfill certain requirements. These are needed in order to utilize FL solutions in practical scenarios, meaning training a model across actual clinics. In this work, we derived these requirements by reviewing the related work, in particular synthetic and real-world implementations.

% own points - Some - Gesprochen mit Max, Klaus, Jonas
In addition, we add further requirements assessed from the experience of three domain experts in the field of FL and DL on medical image data. These requirements are considered by the experts to be relevant for the successful implementation of FL in a medical environment.

% Comparison
Based on the identified requirements, we compare multiple FL solutions, including our own proposed solution based on the JIP. Besides NVIDIA Clara Federated and PySyft, which have already been used in scientific publications (see \Cref{subsec:LitRev} on page \pageref{subsec:LitRev}), we also include the other FL solutions introduced in the related work. These solutions were already examined in earlier scientific publications \citep{Li2019AProtection, He2020FedML:Learning}, have a significant number of GitHub stars, and have an active repository with current contributions. Custom implementations are not considered further for comparison due to their lack of scalability. In order to gather the relevant information, we have intensively studied the provided documentation as well as tutorials and examples, if provided.



\subsection{PySyft Integration into the JIP}
\label{subsec:MethodsPySyftIntegration}
FL experiments can be conducted in various constellations. In order to realize our implementations, we had to make assumptions first.
% Explain general setting
We assume that there are participating clinics and a central instance, each of which hosts its own JIP installation. The communication always takes place via the central instance and not directly between the participants. This is, in particular, also the case with sequential computing plans.
% Data Scientist perspective
We assume that the data scientist who wants to conduct a federated experiment operates from the central instance.

% PySyft Components
The integration of PySyft is limited to its release version 0.2.9 because more recent versions do not yet support the local deployment of the computing instance.
This means that they can only be deployed via a public cloud service, which excludes the more recent PySyft versions from the usage in a secured medical environment.
PySyft comes with three components.
\textit{PySyft-Notebook} is a jupyter-notebook environment with all required packages installed. It runs on the central instance and serves the data scientist as the development environment for his experiment.
\textit{PySyft-Grid} serves as the network to manage the communication. It can be accessed by the data scientist and searched for available data.
\textit{PySyft-Node} is the component that hosts the data, provides references to it in the \textit{PySyft-Grid}, and performs the training computations. A node can be registered in a \textit{PySyft-Grid}, which then enables the data scientist to search and work with the provided references to the data hosted by the node. The hosted data can be marked with identification tags.

All three components are provided by Openmined as ready-to-use container images and are easy to be integrated into the JIP without major adjustments due to the underlying Kubernetes cluster. To open the communication between the different nodes specific ports were excluded from the single sign-on.

% Kombination von beidem
For the experiments in this work, the \textit{PySyft-Node} was integrated into an Airflow DAG as an operator. Together with an operator to load the data into Airflow and another operator to provide the data, they form the DAG in the participating clinics. The data-providing operator performs the image transformations and then sends them all to the \textit{PySyft-Node}. After a given time, the operators stop, and the data is not provided in the \textit{PySyft-Grid} any further. The described workflow is illustrated as DAG in \Cref{fig:Dags} C.

% Training via Jupyter Notebook 
\textit{PySyft-Grid} and \textit{PySyft-Notebook} are hosted on the central instance.
From the notebook, it is then possible to search for the relevant data within \textit{PySyft-Grid} using the assigned identification tags. The received references to the data on the participating clinics can then be used as usual in PyTorch to implement the training logic.


\subsection{JIP Federated}
\label{subsec:MethodsJIPFederated}

% shortly repeat setting
As with the JIP's PySyft integration, we assume a setting of multiple participating clinics and one central instance, all having their individual JIP installed. The data scientist implements and runs the experiment from the central instance.
% Ref to figure
The DAG on the central instance is illustrated in \Cref{fig:Dags} A, the DAG for local training on the participants in \Cref{fig:Dags} B.

% Our JIP Federated implementation is 
We implemented JIP Federated in such way that it is primarily based on the two JIP components Apache Airflow and the object store MinIO as well as their corresponding APIs. Also here, some ports and URLs were whitelisted to enable a communication between the nodes. Different from the PySyft integration, the federated training is scheduled by a DAG on the central instance instead of a jupyter-notebook. An API call to Airflow is used to start the central DAG and thus the federated experiment. With the API call, relevant parameters such as the number of federated rounds, corresponding epochs per round, and learning rate are provided.

%\begin{figure}[h!]
\begin{figure}[htbp]
    \centerline{\includegraphics[width=1\textwidth]{1_Figures/dags.pdf}}
    \caption[DAGs for JIP Federated and the JIP's PySyft integration]{DAGs for JIP Federated (A \& B) and the JIP's PySyft integration (C). (A) DAG on central instance, which schedules the federated experiment and its individual process steps. (B) DAG on the participating institutions, which loads the local data and the consensus model from the central instance to execute the model training. When training is completed, the updated model is saved back to the central instance.  (C) DAG on a participating instance loading and providing its locally available data to the PySyft-Node, which then hosts references to the data into the grid and executes the computations during training. Note that the central instance of the PySyft integration hosts its PySyft-Notebook and PySyft-Grid without being integrated into a DAG.}
\label{fig:Dags}
\end{figure}

% ### scheduler dag ####
The DAG on the central instance controls the processes of the experiment and can be described as follows (see \Cref{fig:Dags} A).
First, depending on the current round, either a new model is initialized  or the model of the previous round is directly stored into the object store.
In case the last round has finished, the final model is saved, and the experiment is completed. Otherwise, it continues with the next operator.
% API Calls & Waiting
After saving the model, API calls are triggered on the participants' sites in order to start the corresponding training DAGs there. The relevant training information is also passed on to the participants via the API call. Subsequently, the workflow waits for the participants to send the models back to the central instance's object store. A regular check is made via the object store's API to see whether all awaited models have arrived.
% aggregation
When this is the case, the following operator performs the model aggregation and constructs the consensus model.
% completion
Before completing the current workflow, the DAG triggers itself for another run. Required data and information, such as the current federated round, are passed on to the next DAG. Thus, one federated round is represented by one run of the DAG on the central instance.

% training dag
At each participating clinic, identical training DAGs are implemented (see \Cref{fig:JIP} B).
The DAG starts with loading the data into Airflow. In parallel, a second operator accesses the object store of the central instance and loads the model from the previous round directly from there.
The operator to train the model brings the loaded data and model together and executes the training. After the local training is completed, the last operator saves the model back into the object store on the central instance.

% Usability
Due to the modularity of the DAGs, the individual operators can be reused for other experiments. In most cases, a developer only needs to customize the experiment-specific operators for model aggregation and model training on the nodes to implement a new experiment.
%Apart from the operators for the experiments, all others are reusable in their current implemented form for further experiments. So a developer only needs to customize the operators for aggregation and training on the nodes to implement the new experiment.


\subsection{Development}
\label{subsec:Development} 

% Intro & General
In the following, the implementations of the JIP's PySyft integration and JIP Federated are described in detail.
The implementations are publicly available on GitHub embedded in the JIP project---the JIP's PySyft integration can be found on the branch \textit{feature/openmined}\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/openmined}\\ commit hash: aab58dac046f22d1b67ee2b49d9f4c72f7b0b69d}, JIP Federated on the branch \textit{feature/federated-training}\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/federated-training}\\ commit hash: 1da26c0dfbe53d3a092762bd8e8a9e2e564148ef}.

% Minio Operator 
A central operator is the \textit{LocalMinioOperator}\footnote{\url{https://github.com/kaapana/kaapana/blob/develop/workflows/airflow-components/plugins/kaapana/operators/LocalMinioOperator.py}}. It serves as an interface between the Airflow environment and the object-store MinIO. The operator can be used for applying the operations \textit{get}, \textit{remove}, and \textit{put} on the buckets and available files. Across both implementations and all experiments, it is used, for instance, to load data into the Airflow environment or to load and store models.
% (\verb|get|, \verb|remove|, \verb|put|)

% ### PySyft ###
\subsubsection{PySyft Integration into the JIP}

% central instance
The three containerized PySyft components (PySyft-Notebook, PySyft-Grid, PySyft-Notebook) are embedded into the JIP as Helm charts\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/openmined/services/applications/openmined}}. This allows to install them as JIP extension via the JIP's GUI. \textit{PySyft-Grid} and \textit{PySyft-Notebook} are installed as such extensions on the central instance.
% participants
On the participants, the \textit{PySyft-Node} is integrated into a DAG\footnote{\url{https://github.com/kaapana/kaapana/blob/feature/openmined/workflows/airflow-components/dags/dag_openmined_provide_data.py}} which is depicted in \Cref{fig:DetailedDAG_PySyft}. In addition to an operator to load the data into the Airflow environment, the following three operators are implemented for this DAG.

% run-node-operator
\textit{RunNodeOperator} is implemented as \textit{Application-Operator}. These operators can launch applications via the Helm API, similar to manually installing an extension. By running this operator, the \textit{PySyft-Node} is started as a temporary application in the JIP. This is done in parallel with the operator described next.

% provide-data-operator
\textit{ProvideDataOperator} starts a Docker container\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/openmined/workflows/processing-container/openmined/provide-data}}, mounting the directory containing the previously loaded data. When the container is launched, it executes its process script. Meaning that data is loaded, transformed, tagged, and then sent to the \textit{PySyft-Node}. A parameter can be set for how long the data should be provided into the node.

% shutdown
When the \textit{ProvideDataOperator} is completed, the subsequent \textit{ShutdownNodeOperator} is launched to call the Helm API to uninstall \textit{PySyft-Node} chart.
From this point on, the node is unsubscribed from the \textit{PySyft-Grid} and its data is not hosted any further. As a last step, the used Airflow directories are emptied.

\begin{figure}[htbp!]
    \centerline{\includegraphics[width=1\textwidth]{1_Figures/DAG_PySyft.pdf}}
    \caption[Detailed illustration of DAG for the JIP's PySyft integration]{Detailed DAG of the JIP's PySyft integration for providing locally available data of participating institutes}
\label{fig:DetailedDAG_PySyft}
\end{figure}

% Operators 
The operators used for the JIP's PySyft integration are can be found in the corresponding repository\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/openmined/workflows/airflow-components/dags/openmined}}.
Exemplary experiment-notebooks are made available together with the Docker image of the \textit{PySyft-Notebook} component\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/openmined/workflows/processing-container/openmined/provide-data}}.



% ### JIP Federated ###
\subsubsection{JIP Federated}
\label{subsubsec:JIPFederated}

As described in \Cref{subsec:MethodsJIPFederated} (see page \pageref{subsec:MethodsJIPFederated}), each experiment using JIP Federated is based on two DAGs\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/federated-training/workflows/airflow-components/dags}}---one implemented on the central instance and the second on the participating institutions. \Cref{fig:DetailedDAG_KaapanaFed} provides a detailed illustration of the two DAGs.
In the following, the implemented operators are described in detail---the source-code is publicly available\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/federated-training/workflows/airflow-components/dags/federated_training}}.
The same applies to the corresponding processing containers\footnote{\url{https://github.com/kaapana/kaapana/tree/feature/federated-training/workflows/processing-container/federated-training}}.
Note that depending on whether \textit{aggregation} or \textit{sequential training} was chosen as computing plan, the behavior of the operators slightly differ.

\textit{EntrypointOperator} is the first operator being executed when the DAG starts. Given the parameter provided by the API call of the data scientist or the previous DAG-run, it determines the next step. 
For the first federated round, the model is initialized. In case the training is completed, the next step is to save the final model and finalize the workflow.
Otherwise, the training continues and the model from the previous round is saved to MinIO.
The routing is realized by the \textit{KaapanaBranchPythonBaseOperator} which branches the workflow depending on the information received from the \textit{EntrypointOperator}.

\textit{ExperimentOperator} is used at two points in the DAG. First, it initializes the model in the first round. Further, it is used for aggregation of models received from the participants. When a \textit{sequential} computing plan is applied, the model is simply loaded and directly saved again into the target directory.
To decide between the tasks to be executed, corresponding parameters are passed when starting the operator.
While all other operators in this DAG are generic, the \textit{ExperimentOperator} is experiment-specific. The corresponding operators for each of the conducted experiments are also available in the repository and can be identified by their names (i.e. \verb|ExperimentsBraTSOperator.py|).

\textit{TriggerRemoteWorkersOperator} triggers the DAGs on the participating institutes. For each participant, an API call is executed which contains the required information and parameters, such as current federated round, number of epochs to train locally, and learning rate to use. In addition, information about the central instance is provided so that an allocation can be done. Note that no model is sent by the central instance.
In parallel, a \textit{LocalMinioOperator} empties the model cache in MinIO, because the new models of the participants are expected there.

\textit{AwaitingModelsOperator} is used to keep the workflow waiting for the models to be received back. It uses the API of MinIO to check every three seconds whether the models of the participants have already arrived in the model cache (which has been emptied before to guarantee that no models from earlier rounds are used). As soon as all newly updated models are available, the loop ends and the operator completed its task.

\textit{TriggerMyselfOperator} is the final operator of the DAG.
It serves for launching the next DAG-run. For this, it transfers the required data, such as the current model

\begin{sidewaysfigure}[htbp]
    \centerline{\includegraphics[width=1\textwidth]{1_Figures/DAG_KaapanaFed.pdf}}
    \caption[Detailed illustration of DAGs for JIP Federated]{DAGs for JIP Federated. (A) DAG on the central instance, scheduling the overall workflow. (B) DAG implemented on the participating instances to perform the model training on locally available data.}
\label{fig:DetailedDAG_KaapanaFed}
\end{sidewaysfigure}

checkpoint, into a newly created folder for the following DAG run. The operator also applies a new API call based on the previous call and therefore triggers the next round. In case of a \textit{sequential} computing plan, this operator determines which participant is next to receive the model for training. In addition, the operator manages the incrementing of the performed federated rounds depending on the applied computing plan.

For the model training on the participating institutes, the following operators are used for the corresponding DAG.
When the DAG is externally triggered by the central instance' \textit{TriggerRemoteWorkersOperator}, two \textit{LocalMinioOperator} are started to load the locally available data and the model into the Airflow environment. Note that the model is loaded directly from the object store of the central instance---this actually implies that the model is not sent to but pulled by the participants.
% Training Operator

\textit{TrainingOperator} launches a Docker container which accesses the loaded data and model.  The container executes a processing script which is similar to a regular PyTorch experiment. Data transformations are applied, and the model is trained for the given number of epochs.
As before, these experiment-specific operators can be identified by their name (i.e. \verb|TrainingBraTSOperator.py|).

% save back to central instance
When the training is completed, another \textit{LocalMinioOperator} is used to store the updated model again directly into the object store of the central instance.
This completes the DAG on the participant. Depending on the configuration, checkpoints of the model can also be saved locally on the participant.


\subsection{Experiments, Algorithms \& Datasets}
\label{subsec:MethodsExperiments}

Two classification experiments were carried out to compare the JIP's PySyft integration and JIP Federated. These experiments aim to compare the runtime and thus the efficiency of the two solutions while achieving similar learning behavior and performance. Additionally, a segmentation experiment on complex multi-modal medical images was conducted to demonstrate the capabilities JIP Federated. 
We demonstrate that it is feasible to perform FL with the JIP on a standard benchmark dataset of medical brain tumor scans.
% General setting with three participants (in all experiments)
For all experiments, we assume that three participating institutes provide their local data. Furthermore, in every federated round one epoch is trained on the available data.

% ### Runtime Experiments ###
The runtime experiments were conducted on two different publicly available datasets: the widely known Modified National Institute of Standards and Technology\footnote{\url{http://yann.lecun.com/exdb/mnist/}} (MNIST) \citep{LeCunYannandCortesCorinnaandBurges2010} dataset, consisting of 70,000 digit handwriting images (60,000 for training and 10,000 for testing), and the paediatric pneumonia dataset\footnote{\url{https://data.mendeley.com/datasets/rscbjbr9sj/3}} originally proposed by \cite{Kermany2018IdentifyingLearning}. It contains paediatric chest radiographs split into 5,232 samples for training and 624 for testing. The data is classified as either normal (with no signs of infection, 1,349 samples) or infected (bacterial or viral pneumonia, 3.883 samples). The test data contains 234 normal samples and 390 samples classified as infected. In order to use the datasets in a federated setting, non-overlapping subsets were formed. Each class in the training data was randomly divided into three parts and distributed among the participants. The test data was held out for final global testing after training. This process was applied on both datasets.
% dataset class distribution 
% total:    5,232 (3,883 pneumonia, 1,349 normal)
% training: ...
% testing:  624 (390 pneumonia, 234 normal)

% Applied transformations
Before the samples of the two datasets were used for training, image transformations were applied. While the MNIST images were only normalized, the following transformations were performed on the pneumonia images.
Image resizing to $256 \times 256$ pixels,
random horizontal flipping,
random vertical flipping,
and image normalization.

For the MNIST experiments, the algorithm and training parameters were adopted from the basic MNIST example of PyTorch\footnote{\url{https://github.com/pytorch/examples/blob/master/mnist/main.py};\\commit hash: 0f0c9131ca5c79d1332dce1f4c06fe942fbdc665}. Training was performed for 14 federated rounds using a batch size of 64 and a learning rate of $10^{-2}$. For the experiment on pneumonia radiographs, the ResNet18 (see \cite{He2016DeepRecognition}) architecture pre-trained on ImageNet was used, following \cite{Kaissis2021End-to-endImaging}. The model was trained for 40 federated rounds with a learning rate of $10^{-4}$ and a batch size of 8 (also used by the baseline for the processing time experiments in \cite{Kaissis2021End-to-endImaging}). Due to PySyft's limitation of not supporting momentum terms in the integrated release version, Stochastic Gradient Descent (SGD) was used as optimizer.
% Kaissis Baseline: "All times shown in white are relative to the baseline for a batch size of 8" - Haben sie als Baseline genutzt um den Zeitaufwand zu vergleichen

% ACCURACY
To measure the performance of the trained models on the test data, the metric \textit{Accuracy} was used. Its value represents the proportion of the correctly classified samples over all samples. The metric is defined as:
\begin{equation}
Accuracy=\frac{TP + TN}{TP + TN + FP + FN} \label{eq:accuracy}
\end{equation}
where TP = True Positive, FP = False Positive, TN = True Negative, FN = False Negative.
% AU-ROC
In addition, the area-under-the-receiver-operator-characteristic (AUROC) is measured for the experiments on the slightly imbalanced pneunomia dataset.
The metric is a performance metric based on the receiver-operator-characteristic (ROC) curve, which represents possible combinations of true-positive-rate (TPR) and the false-postive-rate (FPR).
By mapping the FPR on the x-axis and the TPR on the y-axis, a curve is formed. The area underneath this curve is the AUROC and its value ranges between 0 and 1 (higher is better). 

% Furthermore, the integrated PySyft version does not support graphical processing unit (GPU), thus, the experiments were carried on central processing unit (CPU) resources only.
Furthermore, all JIP instances for the experiments were set up identically, and training was carried out on the central processing unit (CPU). Computing resources of 10,000 Mi\footnote{\unexpanded{$1\ Mi\ (mebibyte) = 1024 \times 1024\ bytes$}} were allocated to each node, respectively training container, by the underlying Kubernetes.


% ### BraTS ###

In the federated segmentation experiment with JIP Federated, a U-Net \citep{Ronneberger2015U-net:Segmentation} was trained to segment diagnosed gliomas in MRI scans of human brains. The U-Net architecture was set up with a channel sequence of ($16, 32, 64, 128, 256$) and convolution strides of size two. The number of residual units was set to two.
The results and training behavior is compared to an experiment where an identical U-Net architecture was trained on the centralized data using the same hyperparameter configuration.
In both cases, the models were trained for 500 epochs, respectively federated rounds. Each federated round consisted of one local epoch on each of the participants.
% GPU
The model training computations were performed on graphical processing units (GPUs).
% Dataset description
The brain dataset from the \textit{Medical Segmentation Decathlon}\footnote{\url{http://medicaldecathlon.com/}} was used. The dataset consists of a subset of the Brain Tumor Segmentation (BraTS) \cite{Menze2015TheBRATS} Challenge 2016 and 2017 data \citep{Antonelli2021TheDecathlon}. Besides unlabeled test samples, it contains 484 multi-modal MRI scans and their corresponding labels. 96 of the labeled samples were separated to serve as test data for measuring the model performance. For the experiment, three participating sites were established on individual server instances. Accordingly, the data was randomly sampled and divided into three non-overlapping subsets. Each instance was provided with 129 samples for training and 32 for testing. For centralized training, the identical data was merged into central datasets and then used for training and testing.
% bezeichnung der Daten <-- clear naming! 
In the remainder of this work, this data will be referred to as BraTS dataset.

% optimizer & reference to appendix for details
To train the model with this data, we used the Adam optimizer \citep{Kingma2015} with a 
learning rate of $10^{-4}$
and weight decay of $10^{-5}$.
% Applied transformations
For the model training, the brain scans undergo the following transformations.
Random vertical flipping,
random spacial cropping ($\textrm{with roi\_size} =  128 \times 128 \times 64$),
intensity normalization,
random intensity shifting,
and random intensity scaling.
% ref to MONAI Tutorial
The parameters of the optimizer, the applied transformations, and the U-Net architecture were adopted from the 3D brain segmentation tutorial\footnote{\url{https://github.com/Project-MONAI/tutorials/blob/master/3d\_segmentation/brats\_segmentation\_3d.ipynb}; commit hash: d340613cea8b0f10b780694c636c1d7ff8f7658d}
%\footnote{https://github.com/Project-MONAI/tutorials}
of the Medical Open Network for AI (MONAI) Project\footnote{\url{https://monai.io/}}. After each federated round, the models are aggregated by applying \textit{Federated Averaging}.

% Optimizer state handling (keep local, agg, reset) --> Das hier ist neu!
When models are aggregated into a consensus model, the question of how to handle the used optimizer arises. For this \textit{optimizer state treatment}, three methods are apparent. As with the model, a consensus optimizer state could be derived from averaging the states of the participants. Alternatively, the optimizer state could be kept on the local instance only or reset for each new federated round. Based on the results shown in the work of \cite{Li2019Privacy-preservingSegmentation}, we decided to also reset the optimizer state before continuing a new federated round.
% sheller gibt nur die Werte für RESET und HOLD an - die sind sehr ähnlich, wie deutlich AGG besser ist wird nicht gesagt. Ich lasse Sheller weg, weil Li klar die Aussage trifft (und auch eben BraTS macht)
% \cite{Li2019Privacy-preservingSegmentation}: "For the treatment of momentum variables, restarting them at each federated round outperforms all the other variants" 

% Dice Score explaining!
As in all BraTS challenges, the \textit{Dice Coefficient} (DC) was used for the quantitative performance evaluation of the model.
This metric measures the overlap of two binary segmentation masks and ranges from 0 to 1 (higher is better). DC is defined as:
\begin{equation}
DC=\frac{2 |P \cap T|}{|P| + |T|} \label{eq:dice}
\end{equation}
where P is the prediction and T the ground truth mask. The DC is measured for all three (overlapping) tumor regions\footnote{In the medical segmentation decathlon, the label convention slightly differs from BraTS.}:
\begin{itemize}
    \item Enhancing tumor (ET): label 2
    \item Tumor core (TC): labels 2 and 3
    \item Whole tumor (WT): labels 1, 2 and 3
\end{itemize}

\begin{comment}
\begin{enumerate}
    \item Enhancing tumor (ET): label 2
    \item Tumor core (TC): labels 2 and 3
    \item Whole tumor (WT): labels 1, 2 and 3
\end{enumerate}
\end{comment}

% Table summerazis the chosesn parameter configurartions for all conducted experiments.
\Cref{tab:TrainingParameters} summarizes the chosen parameter configurations for all conducted experiments.
Note that apart from the listed parameters, the Adam optimizer was used with its PyTorch default values for the segmentation experiment.

\begin{table}[htbp]
\begin{adjustbox}{width=1.0\textwidth}
  \centering
  \begin{tabular}{ccccccc}
  Experiment & Federated Rounds & Epochs per Round & Optimizer & Batch Size & Learning Rate & Weight Decay \\
  \hline \\[-2.5ex] %[-1.5ex]
  MNIST     & 14    & 1 & SGD   & 64 & $10^{-2}$ & - \\
  Pneunomia & 40    & 1 & SGD   & 8  & $10^{-4}$ & - \\
  BraTS     & 500   & 1 & Adam  & 2  & $10^{-4}$ & $10^{-5}$ \\
 \end{tabular}
 \end{adjustbox}
 \caption[Training parameters for conducted FL experiments]{Training parameters for conducted FL experiments on MNIST, pneumonia and BraTS data}
 \label{tab:TrainingParameters}
\end{table}
